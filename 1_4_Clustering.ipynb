{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Clustering](https://scikit-learn.org/stable/modules/clustering.html) is an unsupervised technique with the aim of producing labelled groups. It is often a good way to examine unlabelled data to discover new insights into its substructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means\n",
    "\n",
    "The k-means heuristic acts on the raw data. We need to provide the number of clusters, *k*.\n",
    "\n",
    "It is an iterative procedure that first guesses the locations of *k* cluster centres, assigns data points to the cluster with the nearest centre, then updates the locations of the centres.\n",
    "\n",
    "k-means works well for compact, well-separated clusters with similar variance but may perform poorly in other cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# generate synthetic two-dimensional data\n",
    "X, y = make_blobs(random_state=1)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the clustering model\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster memberships:\\n{}\".format(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, alpha=0.5)\n",
    "plt.scatter(\n",
    "    kmeans.cluster_centers_[:, 0], \n",
    "    kmeans.cluster_centers_[:, 1], \n",
    "    c='k',\n",
    "    marker='^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k too low or too high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# using two cluster centers:\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "assignments = kmeans.labels_\n",
    "\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=assignments, alpha=0.5)\n",
    "\n",
    "# using five cluster centers:\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X)\n",
    "assignments = kmeans.labels_\n",
    "\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=assignments, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters with different densities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_varied, y_varied = make_blobs(\n",
    "    n_samples=200,\n",
    "    cluster_std=[0.5, 2.2, 0.5],\n",
    "    random_state=170)\n",
    "y_pred = KMeans(\n",
    "    n_clusters=3, \n",
    "    random_state=0).fit_predict(X_varied)\n",
    "\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anisotropic data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random cluster data\n",
    "X, y = make_blobs(random_state=170, n_samples=600)\n",
    "rng = np.random.RandomState(74)\n",
    "\n",
    "# transform the data to be stretched\n",
    "transformation = rng.normal(size=(2, 2))\n",
    "X = np.dot(X, transformation)\n",
    "\n",
    "# cluster the data into three clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# plot the cluster assignments and cluster centers\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, alpha=0.5)\n",
    "plt.scatter(\n",
    "    kmeans.cluster_centers_[:, 0], \n",
    "    kmeans.cluster_centers_[:, 1],\n",
    "    marker='^', c='k')\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex cluster shapes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic two_moons data (with less noise this time)\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# cluster the data into two clusters\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "# plot the cluster assignments and cluster centers\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, alpha=0.5 )\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], \n",
    "            kmeans.cluster_centers_[:, 1],\n",
    "            marker='^',\n",
    "            c='k')\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative clustering\n",
    "\n",
    "This is a family of methods that progressively combine data points to produce a hierarchical clustering.\n",
    "\n",
    "We can choose the *linkage criterion* to specify how to decide which pair of clusters is considered \"most similar\".\n",
    "\n",
    "* *Complete linkage* merges clusters that have the smallest maximum distance between their data points.\n",
    "* *Average linkage* merges clusters that have the smallest mean pairwise distance between their data points.\n",
    "* *Ward linkage* merges clusters so that the total within-cluster variance increases the least. This is the default behaviour\n",
    "\n",
    "We need to provide a value for the desired number of clusters that will be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "X, y = make_blobs(random_state=1)\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=assignment, alpha=0.5)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look in more detail at how this works, using a smaller data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dendrogram function and the ward clustering function from SciPy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "\n",
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "for i in range(len(X)):\n",
    "    plt.text(X[i, 0], X[i, 1], str(i),\n",
    "             fontdict={'size': 12})\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the ward clustering to the data array X\n",
    "# The SciPy ward function returns an array that specifies the distances\n",
    "# bridged when performing agglomerative clustering\n",
    "linkage_array = ward(X)\n",
    "linkage_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we plot the dendrogram for the linkage_array containing the distances\n",
    "# between clusters\n",
    "dendrogram(linkage_array)\n",
    "\n",
    "# mark the cuts in the tree that signify two or three clusters\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "ax.plot(bounds, [7.25, 7.25], '--', c='k')\n",
    "ax.plot(bounds, [4, 4], '--', c='k')\n",
    "\n",
    "ax.text(bounds[1], 7.25, ' two clusters', va='center', fontdict={'size': 15})\n",
    "ax.text(bounds[1], 4, ' three clusters', va='center', fontdict={'size': 15})\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "DBSCAN (Density-based spectral clustering of applications with noise) is another useful algorithm for clustering. It has the benefit that the number of clusters does not need to be specified in advance. It can capture clusters with complex shapes, and also identify points that do not belong to any cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=0, n_samples=12)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X)\n",
    "print(\"Cluster memberships:\\n{}\".format(clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster label `-1` means \"noise\"!\n",
    "\n",
    "There were no regions of the data that were considered \"dense\", under the default settings.\n",
    "\n",
    "Let's try again on the \"moons\" data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# Rescale the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "# plot the cluster assignments\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of clustering\n",
    "\n",
    "In general it is difficult to assess the \"quality\" of the clusters that we obtain. However, in some circumstances we can calculate meaningful measures of quality.\n",
    "\n",
    "#### Evaluation with ground truth\n",
    "\n",
    "For data sets that have associated class labels as targets (i.e. not features used in the clustering), we can use measures such as the *Adjusted Rand Index* (ARI) to evaluate agreement with the ground truth.\n",
    "\n",
    "evaluation without ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# Rescale the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "# make a list of algorithms to use\n",
    "algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n",
    "              DBSCAN()]\n",
    "\n",
    "# create a random cluster assignment for reference\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "random_clusters = random_state.randint(low=0, high=2, size=len(X))\n",
    "\n",
    "# plot random assignment\n",
    "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n",
    "                )\n",
    "axes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\n",
    "        adjusted_rand_score(y, random_clusters)))\n",
    "\n",
    "for ax, algorithm in zip(axes[1:], algorithms):\n",
    "    # plot the cluster assignments and cluster centers\n",
    "    clusters = algorithm.fit_predict(X_scaled)\n",
    "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters)\n",
    "    ax.set_title(\"{} - ARI: {:.2f}\".format(\n",
    "        algorithm.__class__.__name__,                      \n",
    "        adjusted_rand_score(y, clusters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation without ground truth\n",
    "\n",
    "When class labels are not available, we can use other measures such as the *silhouette score* to get information about the relative compactness of the clusters found. \n",
    "\n",
    "However, in practice these approaches can often be misleading, as the most compact clusterings do not always coincide with the \"best\" solutions for more complex shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# rescale the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3),\n",
    "                         subplot_kw={'xticks': (), 'yticks': ()})\n",
    "\n",
    "# create a random cluster assignment for reference\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "random_clusters = random_state.randint(low=0, high=2, size=len(X))\n",
    "\n",
    "# plot random assignment\n",
    "axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n",
    "                )\n",
    "axes[0].set_title(\"Random assignment: {:.2f}\".format(\n",
    "    silhouette_score(X_scaled, random_clusters)))\n",
    "\n",
    "algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n",
    "              DBSCAN()]\n",
    "\n",
    "for ax, algorithm in zip(axes[1:], algorithms):\n",
    "    clusters = algorithm.fit_predict(X_scaled)\n",
    "    # plot the cluster assignments and cluster centers\n",
    "    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], \n",
    "               c=clusters)\n",
    "    ax.set_title(\"{} : {:.2f}\".format(\n",
    "        algorithm.__class__.__name__,\n",
    "        silhouette_score(X_scaled, clusters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try clustering the `digits` dataset with a method of your choice.\n",
    "\n",
    "Give an evaluation of performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
