{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "Cross-validation is essential in model development - it allows us to compare the performance of alternative algorithms and different settings for model hyperparameters, *without* making use of the test data. This is very important so that we can obtain an accurate assessment of the final model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KFold` is a simple way to get the data indices for cross-validation, which we can loop over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# Using only the first 250 data points\n",
    "X = diabetes.data[:250]\n",
    "y = diabetes.target[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "for train, test in kf.split(X):\n",
    "    print(\"training set indices:\")\n",
    "    print(train)\n",
    "    print(\"test set indices:\")\n",
    "    print(test)\n",
    "    lm.fit(X[train], y[train])\n",
    "    y_pred = lm.predict(X[test])\n",
    "    print(\"r2 = %.2f\" % r2_score(y[test],y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to calculate a metric, there is another convenient function `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# start with the untrained model\n",
    "lm = LinearRegression()\n",
    "\n",
    "# a 5-fold cross-validation, scored using r2\n",
    "score = cross_val_score( lm,X,y,cv=5,scoring='r2' )\n",
    "print(\"Cross-validated r2:\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would usually quote the mean score under cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean r2 =\", np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard deviation of the cross-validation scores is also useful as an estimate of the error compared to the true performance on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sd =\", np.std(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the basic *k*-fold cross-validation, there are many alternative procedures that may be suitable depending on the structure of your particular data set. \n",
    "\n",
    "For example, there may be definable subgroups within the data that we might want to leave out of training one at a time, to assess how good the predictor is at extrapolating beyond known groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A cross-validated ROC curve\n",
    "\n",
    "With cross-validation, each element of the training data set is also used in validation - this is a great way to get a robust overview of predictive performance for our methodology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "steel = fetch_openml(name='steel-plates-fault', version=1, parser='auto')\n",
    "X, y = steel.data.to_numpy(), steel.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import RocCurveDisplay, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "# StratifiedKFold maintains relative proportions of classes in each fold.\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=4)\n",
    "\n",
    "# We use a penalised logistic regression model\n",
    "classifier = LogisticRegression(penalty='l2', solver='liblinear')\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "\n",
    "# A linspace for the FPR values to allow us to calculate mean behaviour\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Create the axis\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for fold, (train, test) in enumerate(cv.split(X, y)):\n",
    "    classifier.fit(X[train], y[train])\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        classifier,\n",
    "        X[test],\n",
    "        y[test],\n",
    "        name=f\"ROC fold {fold}\",\n",
    "        # Make the line partially transparent\n",
    "        alpha=0.3,\n",
    "        # Make the linewidth small\n",
    "        lw=1,\n",
    "        # Plotting onto the existing axis\n",
    "        ax=ax,\n",
    "        plot_chance_level=(fold == n_splits - 1),\n",
    "    )\n",
    "\n",
    "    # We calculate interpolated values of TPR at the chosen FPR values\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    # Fix the first TPR as 0\n",
    "    interp_tpr[0] = 0.0\n",
    "    # Keep the TPR values for this split\n",
    "    tprs.append(interp_tpr)\n",
    "    # Keep the AUC value for this split\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "# Now calculate mean TPR over all the splits\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "\n",
    "# Fix the last TPR as 1\n",
    "mean_tpr[-1] = 1.0\n",
    "# Calculate AUC for the mean ROC\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "# Calculate std from the individual AUCs\n",
    "std_auc = np.std(aucs)\n",
    "# Plot the line for mean ROC\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Calculate std of TPR at each FPR (x-axis) value\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "# Use this to shade a region around the mean ROC curve\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=f\"Mean ROC curve with variability')\",\n",
    ")\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use k-fold cross-validation to estimate the performance of a MLPRegressor on the `wine_quality_white` dataset. Use a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does adding a second hidden layer appear to improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ilpd` dataset contains data from 416 liver patient records (class 1) and 167 non liver patient records (class 2). Plot a cross-validated ROC curve for a random forest classifier on these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilpd = fetch_openml(name='ilpd', version=1, parser='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilpd.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ilpd.target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
