{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many metrics that we may want to use to evaluate performance of supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Evaluating Classifiers](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.metrics` provides most of the commonly-used metrics.\n",
    "\n",
    "Some of these are restricted to binary classifiers, but others are also defined for multiclass (several possible values for `y`) and/or multilabel (potential for multiple simultaneous values for `y`) problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda14f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svc = SVC(probability=True, random_state=42)\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print( classification_report(y_test,y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The receiver-operating characteristic (ROC) curve gives a useful visual evaluation for any method that can return probabilities or prediction scores. The following code works for binary classification (there is an alternative for multiclass classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "svc_disp = RocCurveDisplay.from_estimator(svc, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the area under the curve (AUC) as a metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "probs = svc.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,probs[:,1])\n",
    "print(\"AUC =\",auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f92fd9",
   "metadata": {},
   "source": [
    "The best possible classifier would have AUC = 1. A binary classifier with random guessing has AUC = 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc32d4",
   "metadata": {},
   "source": [
    "We can compare classifiers visually by plotting multiple ROC curves on the same axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2, metric='minkowski', p=2)\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "knn_disp = RocCurveDisplay.from_estimator(knn, X_test, y_test)\n",
    "ax = plt.gca()\n",
    "svc_disp.plot(ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision-recall (PR) curve is also a useful evaluation for tasks where we are most interested in eliminating false positives, e.g. screening a population for a disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(svc, X_test, y_test)\n",
    "#pre,rec,thresholds = precision_recall_curve(y_test,probs[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [weighted average precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) over this curve is often quoted as a metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "avg_pre = average_precision_score(y_test, probs[:,1])\n",
    "print(\"Average precision =\",avg_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Evaluating Regressors](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n",
    "\n",
    "Once again, there are several metrics for evaluation of regression - the user guide has full details for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "nn = MLPRegressor(hidden_layer_sizes=(100),max_iter=10000)\n",
    "nn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,max_error,r2_score\n",
    "\n",
    "print(\"Mean Absolute Error, MAE = %.2f\" % mean_absolute_error(y_test, y_pred))\n",
    "print(\"Mean squared error, MSE = %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "print(\"Max Error = %.2f\" % max_error(y_test, y_pred))\n",
    "print(\"Coefficient of determination, r2 = %.2f\" % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ROC curves to compare the performance of a Decision Tree and Logistic Regressor on the `breast_cancer` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does a random forest outperform a simple linear model for the `wine_quality_white` dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f909e96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
