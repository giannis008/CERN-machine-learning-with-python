{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Regression\n",
    "\n",
    "In this notebook we will discuss some examples of supervised learning algorithms applied to regression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example data\n",
    "\n",
    "We will use the *autoMpg* dataset as an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mpg = fetch_openml(name='autoMpg', version=1, parser='auto')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(mpg.data, mpg.target, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we have some preprocessing to do on this dataset. \n",
    "\n",
    "Because we need to apply the same transformation to both the training and testing data, it will be convenient to wrap these steps using [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#columntransformer) and [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Defines preprocessing transformations for specified columns\n",
    "ct = ColumnTransformer([('encode', OneHotEncoder(), ['origin']),\n",
    "                        ('impute', IterativeImputer(), ['horsepower'])],\n",
    "                       remainder='passthrough') \n",
    "\n",
    "# Defines individual steps in a workflow\n",
    "pipe = Pipeline([('preprocessing', ct),\n",
    "                 ('scaling', StandardScaler())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can fit the whole pipeline in one step (using training data only)...\n",
    "pipe.fit(X_train)\n",
    "\n",
    "# ... and use it transform the features (both the training and the testing sets)\n",
    "X_train_ = pipe.transform(X_train)\n",
    "X_test_ = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to do some regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "\n",
    "Linear regression uses Ordinary Least Squares to find the optimal model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Use only one feature\n",
    "feature_number = 3\n",
    "X_train_1 = X_train_[:, np.newaxis, feature_number]\n",
    "X_test_1 = X_test_[:, np.newaxis, feature_number]\n",
    "\n",
    "lm_1 = LinearRegression()\n",
    "lm_1.fit(X_train_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm_1.predict(X_train_1)\n",
    "\n",
    "print(\"Coefficients: \\n\", lm_1.coef_)\n",
    "\n",
    "plt.scatter(X_train_1, y_train, c='m' )\n",
    "plt.plot(X_train_1, y_pred, c=\"k\")\n",
    "plt.title(\"linear model + training data\")\n",
    "plt.xlabel(pipe.get_feature_names_out()[feature_number])\n",
    "plt.ylabel(\"mpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm_1.predict(X_test_1)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error, MSE = %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination, r2 = %.2f\" % r2_score(y_test, y_pred))\n",
    "\n",
    "plt.scatter(X_test_1, y_test, c='c')\n",
    "plt.plot(X_test_1, y_pred, c='k')\n",
    "plt.title(\"linear model + test data\")\n",
    "plt.xlabel(pipe.get_feature_names_out()[feature_number])\n",
    "plt.ylabel(\"mpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the model performance by including all the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: \\n\", lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm.predict(X_test_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error, MSE = %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination, r2 = %.3f\" % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularisation\n",
    "\n",
    "OLS places no restrictions on the values of the coefficients. \n",
    "\n",
    "However, to avoid overfitting we may prefer to find a model that still fits the training data well, but has coefficients that are (mostly) small. This idea is called *regularisation*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [*ridge regression*](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification), we add an additional L2 penalty on top of the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.2)\n",
    "ridge.fit(X_train_,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: \\n\", ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ridge.predict(X_test_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error, MSE = %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination, r2 = %.3f\" % r2_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [*LASSO regression*](https://scikit-learn.org/stable/modules/linear_model.html#lasso), we use an L1 penalty instead. This tends to produce coefficients that are exactly 0, hence they can be removed from the model. LASSO can therefore be used as an automated form of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train_,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: \\n\", lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso.predict(X_test_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error, MSE = %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination, r2 = %.3f\" % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear regression\n",
    "\n",
    "Note that many supervised learning algorithms can be used for both classification and regression with only minor adaptations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error, MSE = %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination, r2 = %.3f\" % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "tree.fit(X_train_,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error, MSE = %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination, r2 = %.3f\" % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "plot_tree(tree, \n",
    "          impurity=False, \n",
    "          filled=True,\n",
    "          max_depth=2, \n",
    "          feature_names=pipe.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100,random_state=0)\n",
    "rf.fit(X_train_,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error, MSE = %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination, r2 = %.3f\" % r2_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "nn = MLPRegressor(hidden_layer_sizes=(100),max_iter=10000)\n",
    "nn.fit(X_train_,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(X_test_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error, MSE = %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination, r2 = %.3f\" % r2_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_train_,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svr.predict(X_test_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error, MSE = %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination, r2 = %.3f\" % r2_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Train a regressor of your choice on the `wine_quality_white` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "w = fetch_openml(name='wine-quality-white',version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your model do better than Ordinary Least Squares?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
