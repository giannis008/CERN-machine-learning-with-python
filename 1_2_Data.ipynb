{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data\n",
    "\n",
    "To get started, let's get hold of some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)\n",
    "\n",
    "We have some small demonstration datasets immediately available in `sklearn.datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is returned as a `Bunch` object. This is similar to a dictionary, but also allows values to be referenced as attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"filename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `data` and the `target` are provided separately as numpy arrays. Each row is an observation (i.e. a data point) and each column is a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `DESCR` attribute to find out more about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting testing from training data\n",
    "\n",
    "Before we even look at the data, it is good practice to split off a test dataset that will remain unseen until we are ready for final evaluation of our models.\n",
    "\n",
    "sklearn has a convenient function `train_test_split` that will create a randomised 75%:25% split of training:testing data. The `X` values are the features and the `y` values are the target.\n",
    "\n",
    "Notice that we can \"seed\" the random number generator to create deterministic output - this can be helpful during code development as it means our results will not change between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising data\n",
    "\n",
    "Before we start, it is good practice to take a look at the general form of the data to identify any inconsistencies or errors.\n",
    "\n",
    "We can use the `scatter_matrix` from pandas to create a pairwise matrix of scatter plots together with histograms for the individual features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create dataframe from data in X_train\n",
    "# label the columns using the strings in iris_dataset.feature_names\n",
    "iris_dataframe = pd.DataFrame(X_train, columns=iris.feature_names)\n",
    "\n",
    "# create a scatter matrix from the dataframe, color by y_train\n",
    "pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15),\n",
    "                           marker='o', hist_kwds={'bins': 20}, s=60,\n",
    "                           alpha=.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Real-world datasets](https://scikit-learn.org/stable/datasets/real_world.html)\n",
    "\n",
    "In addition to the toy data, scikit-learn has loader functions for some commonly used larger sets. For example, the [Olivetti faces](https://scikit-learn.org/stable/datasets/real_world.html#the-olivetti-faces-dataset) dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "olivetti = fetch_olivetti_faces()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olivetti.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
       "        3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "       11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
       "       13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15,\n",
       "       15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
       "       18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20,\n",
       "       20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22,\n",
       "       22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "       23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25,\n",
       "       25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27,\n",
       "       27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "       28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30,\n",
       "       30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32,\n",
       "       32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "       34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35,\n",
       "       35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37,\n",
       "       37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39,\n",
       "       39, 39, 39, 39, 39, 39, 39, 39, 39])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olivetti.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olivetti.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olivetti.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(olivetti.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Data generators](https://scikit-learn.org/stable/datasets/sample_generators.html)\n",
    "\n",
    "Sometimes we want to generate synthetic data to test clustering and regression methods. scikit-learn provides a number of helpful functions to do this for us.\n",
    "\n",
    "For example, we can create a classification dataset consisting of a number of blobs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# 100 data points, 2 features, 3 blobs\n",
    "blobs_X, blobs_y = make_blobs(100, 2, centers=3)\n",
    "\n",
    "# split off a test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(blobs_X, blobs_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:,0],X_train[:,1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [external datasets](https://scikit-learn.org/stable/datasets/loading_other_datasets.html#loading-from-external-datasets)\n",
    "\n",
    "There are of course many other sources of data that we might want to use.\n",
    "\n",
    "scikit-learn provides a direct interface to the [OpenML](https://www.openml.org/home) repository, so it is very easy to make use of these datasets in your work. See [here](https://scikit-learn.org/stable/datasets/loading_other_datasets.html#downloading-datasets-from-the-openml-org-repository) for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mice = fetch_openml(name='miceprotein', version=4)\n",
    "\n",
    "mice.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load data from a CSV file, we would use the pandas functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codon = pd.read_csv(\"codon_usage.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a DataFrame for the features\n",
    "codon_X = codon.iloc[:,5:]\n",
    "\n",
    "codon_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a Series for the target\n",
    "codon_y = codon.iloc[:,0]\n",
    "\n",
    "codon_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now work in scikit-learn with these pandas objects \n",
    "# in exactly the same way as for the numpy arrays\n",
    "X_train, X_test, y_train, y_test = train_test_split(codon_X, codon_y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other data resources\n",
    "\n",
    "Some other repositories you may find useful:\n",
    "\n",
    "https://paperswithcode.com/datasets\n",
    "\n",
    "https://archive.ics.uci.edu/ml/index.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "There may be several preprocessing steps that we need to complete before data is ready to use in our chosen method. Below are a few processes that are commonly applied.\n",
    "\n",
    "Importantly, note that the any transformations of the features (e.g. imputation or standardisation discussed below) should be calculated on the *training* data only, after which the same transformation is applied to both the training and testing data. \n",
    "\n",
    "If we were to fit to the *entire* data set, after transformation our testing data would be contaminated with information from the training data, which is what we want to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following table of features describing some students:\n",
    "\n",
    "* Age\n",
    "* Subject of current degree course\n",
    "* Level of current degree course\n",
    "* Height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = pd.DataFrame(\n",
    "            [\n",
    "              [25,\"Chemistry\",\"PhD\",1.50],\n",
    "              [32,\"Physics\",\"MSc\",1.67],\n",
    "              [20,\"Mathematics\",\"BSc\",1.69],\n",
    "              [22,\"Mathematics\",\"PhD\",1.58],\n",
    "              [np.nan,\"Physics\",\"PhD\",1.70],\n",
    "              [25,\"Physics\",\"PhD\",1.82],\n",
    "              [np.nan,\"Mathematics\",\"BSc\",1.49],\n",
    "              [19,\"Chemistry\",\"BSc\",1.80]\n",
    "             ], \n",
    "             columns=[\"age\",\"subject\",\"level\",\"height\"]\n",
    "          )\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Encoding categorical features](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features)\n",
    "\n",
    "Categorical datatypes need to be transformed to numbers before scikit-learn can use them as features.\n",
    "\n",
    "For binary features and features whose categories have a natural ordering, this can be done by assigning an integer value to each category. This is called *ordinal encoding*.\n",
    "\n",
    "The feature `level` has a natural ordering, so we can use the `OrdinalEncoder` to encode it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the level column as a pandas DataFrame\n",
    "level = X_train[[\"level\"]]\n",
    "\n",
    "level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# we provide the ordered categories to the encoder\n",
    "enc = OrdinalEncoder(categories=[[\"BSc\",\"MSc\",\"PhD\"]])\n",
    "\n",
    "# fit the encoder to the data\n",
    "enc.fit(level)\n",
    "\n",
    "# encode the data\n",
    "level_enc = enc.transform(level)\n",
    "\n",
    "print(level_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the original DataFrame\n",
    "X_train_enc = X_train.copy()\n",
    "\n",
    "# replace the \"level\" column with the encoded version\n",
    "X_train_enc[\"level\"] = level_enc\n",
    "\n",
    "X_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although ordinal encoding is suitable for many categorical features, it may produce poor performance when the data type is nominal, i.e. where no meaningful ordering exists for the allowed values.\n",
    "\n",
    "A better solution in these cases is to use *one-hot encoding* to replace a single integer feature with multiple binary features.\n",
    "\n",
    "We will use a `OneHotEncoder` to encode the `subject` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the subject column as a pandas DataFrame\n",
    "subject = X_train[[\"subject\"]]  \n",
    "\n",
    "subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#setting sparse=False means that enc.transform() will return an array\n",
    "enc = OneHotEncoder( sparse=False )\n",
    "\n",
    "# fit the encoder to the data\n",
    "enc.fit(subject)\n",
    "\n",
    "# encode the data\n",
    "subject_enc = enc.transform(subject)\n",
    "\n",
    "print(subject_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = pd.DataFrame( subject_enc, columns= \"subject_\" + enc.categories_[0] )\n",
    "\n",
    "new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the original \"subject\" feature and add the new features\n",
    "X_train_enc = X_train_enc.drop(\"subject\",axis=1).join(new_columns)\n",
    "\n",
    "X_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Imputation of missing values](https://scikit-learn.org/stable/modules/impute.html)\n",
    "\n",
    "Real-world datasets often contain missing values, represented in data as `\"?\"` or `np.nan`.\n",
    "\n",
    "We might choose to ignore (drop) rows that contain missing values. However, this wastes the rest of the information in that row.\n",
    "\n",
    "It may be more desirable to insert a guess in place of the missing values. scikit-learn provides a couple of methods to do this.\n",
    "\n",
    "The simplest approach is to use the mean of the column values in place of any unknown value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer()\n",
    "imp.fit(X_train_enc)\n",
    "X_imp = imp.transform(X_train_enc)\n",
    "\n",
    "X_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated approach uses iterative regression modelling to try to guess the unknown values, based on the other features seen in that row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imp = IterativeImputer()\n",
    "imp.fit(X_train_enc)\n",
    "X_imp = imp.transform(X_train_enc)\n",
    "\n",
    "X_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Standardisation](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)\n",
    "\n",
    "Once we have encoded categorical features and imputed any missing values, it may be necessary to center the data and transform it so that all features have equal variance. This is a requirement for some machine learning methods to operate correctly.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_imp)\n",
    "\n",
    "X_scaled = scaler.transform(X_imp)\n",
    "\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature in the scaled data has zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.mean( axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.var( axis=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this basic type of standardisation will cause problems for sparse data sets and data containing outlier values - there are other functions that implement alternative scaling procedures recommended for these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Download the `autoMpg` dataset from OpenML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mpg = fetch_openml(name='autoMpg',version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.openml.org/d/196 for a description of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `origin` feature is a nominal value coded as an integer.\n",
    "Use one-hot encoding to turn this column into multiple binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `horsepower` feature has 6 missing values. Can you impute them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, standardise the dataset using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
